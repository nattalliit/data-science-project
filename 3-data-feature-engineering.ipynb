{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Insurance Claims Fraud Detection - Feature Engineering\r\n",
    "\r\n",
    "After completing the initial data cleansing in notebook `1-data-cleansing`, the next step is to manipulate the data and derive features.\r\n",
    "\r\n",
    "## 1.2 Feature extraction\r\n",
    "\r\n",
    "The following none exhaustive list gives you some guidelines for __feature transformation__:\r\n",
    "\r\n",
    "* __Imputing__ <br>\r\n",
    "Fill missing values based on their value distribution, as some algorithms are sensitive to missing data\r\n",
    "* __Imputed time-series quantization__ <br>\r\n",
    "For time series data with measurements at different timestamps, quantize measurements to a common interval and impute corresponding values\r\n",
    "* __Scaling / Normalizing / Centering__ <br>\r\n",
    "Center data around zero and scale values to have a standard deviation of one to address algorithm sensitivity to differences in value ranges\r\n",
    "*  __Filtering__ <br>\r\n",
    "Delete low-quality records if imputing values doesn't yield satisfactory results\r\n",
    "* __Discretizing__ <br>\r\n",
    "Convert continuous fields into discrete categories, as discrete age ranges may perform better than continuous values, particularly with simpler models or smaller datasets\r\n",
    "\r\n",
    "The following none exhaustive list gives you some guidelines for __feature creation__:\r\n",
    "* __One-hot-encoding__ <br>\r\n",
    "Transform categorical integer features into \"one-hot\" vectors, adding additional columns for each distinct category\r\n",
    "* __Time-to-Frequency transformation__ <br>\r\n",
    "Convert time-series or sequence data from the time domain to the frequency domain using techniques like FFT (Fast Fourier Transformation)\r\n",
    "* __Month-From-Date__ <br>\r\n",
    "Create an additional feature indicating the month independently from the date to capture seasonal aspects, and optionally further discretize into quarters\r\n",
    "* __Aggregate-on-Target__ <br>\r\n",
    "Aggregate fields with respect to the target variable or other relevant fields to improve performance. For example, count the number of data points per ZIP code or calculate the median of all values by geographical region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!pip install pandas-profiling\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, path):\n",
    "    # Prepend dtypes to the top of df\n",
    "    df2 = df.copy()\n",
    "    df2.loc[-1] = df2.dtypes\n",
    "    df2.index = df2.index + 1\n",
    "    df2.sort_index(inplace=True)\n",
    "    # Then save it to a csv\n",
    "    df2.to_csv(path, index=False)\n",
    "    \n",
    "def read_csv(path):\n",
    "    # Read types first line of csv\n",
    "    dtypes = {key:value for (key,value) in pd.read_csv(path,    \n",
    "              nrows=1).iloc[0].to_dict().items() if 'date' not in value}\n",
    "\n",
    "    parse_dates = [key for (key,value) in pd.read_csv(path, \n",
    "                   nrows=1).iloc[0].to_dict().items() if 'date' in value]\n",
    "    # Read the rest of the lines with the types from above\n",
    "    return pd.read_csv(path, dtype=dtypes, parse_dates=parse_dates, skiprows=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 36)\n",
      "months_as_customer                      int64\n",
      "age                                     int64\n",
      "policy_bind_date               datetime64[ns]\n",
      "policy_state                         category\n",
      "policy_csl                           category\n",
      "policy_deductable                       int64\n",
      "policy_annual_premium                 float64\n",
      "umbrella_limit                          int64\n",
      "insured_zip                             int64\n",
      "insured_sex                          category\n",
      "insured_education_level              category\n",
      "insured_occupation                   category\n",
      "insured_hobbies                      category\n",
      "insured_relationship                 category\n",
      "capital-gains                           int64\n",
      "capital-loss                            int64\n",
      "incident_date                  datetime64[ns]\n",
      "incident_type                        category\n",
      "collision_type                       category\n",
      "incident_severity                    category\n",
      "authorities_contacted                category\n",
      "incident_state                       category\n",
      "incident_city                        category\n",
      "incident_hour_of_the_day                int64\n",
      "number_of_vehicles_involved             int64\n",
      "property_damage                      category\n",
      "bodily_injuries                         int64\n",
      "witnesses                               int64\n",
      "police_report_available              category\n",
      "total_claim_amount                      int64\n",
      "injury_claim                            int64\n",
      "property_claim                          int64\n",
      "vehicle_claim                           int64\n",
      "auto_make                            category\n",
      "auto_year                               int64\n",
      "fraud_reported                          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the data with dtypes\n",
    "data = read_csv('data/insurance_claims_clean.csv')\n",
    "print(data.shape)\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = data['fraud_reported']\n",
    "X = data.drop('fraud_reported', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformation:\n",
    "__Imputing__ <br>\n",
    "Some algorithms are very sensitive to missing values. Therefore, imputing allows for filling of empty fields based on its value distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NA. No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imputed time-series quantization__ <br>\n",
    "Time series often contain streams with measurements at different timestamps. Therefore, it is beneficial to quantize measurements to a common “heart beat” and impute the corresponding values. This can be done by sampling from the source time series distributions on the respective quantized time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NA. Data is not longitudinal time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Filtering__ <br>\n",
    "Sometimes imputing values doesn’t perform well, therefore deletion of low quality records is a better strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discretizing__ <br>\n",
    "Continuous fields might confuse the model, e.g. a discrete set of age ranges sometimes performs better than continuous values, especially on smaller amounts of data and with simpler models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin age into young driver, adult, senior (ordered categorical)\n",
    "bins = [18,25,45,64]\n",
    "\n",
    "X_train['age_bins'] = pd.cut(X_train['age'], bins)\n",
    "X_test['age_bins'] = pd.cut(X_test['age'], bins)\n",
    "# Categories (3, interval[int64]): [(0, 25] < (25, 45] < (45, 64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create additional 'other' column if hobbies are not chess and cross-fit\n",
    "X_train['insured_hobbies'] = X_train['insured_hobbies'].apply(lambda x: 'other' if x!='chess' and x!='cross-fit' else x)\n",
    "X_test['insured_hobbies'] = X_test['insured_hobbies'].apply(lambda x: 'other' if x!='chess' and x!='cross-fit' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'other' column if insured_occupation is not exec-managerial\n",
    "# ToDo - test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scaling / Normalizing / Centering__ <br>\n",
    "Some algorithms are very sensitive differences in value ranges for individual fields. Therefore, it is best practice to center data around zero and scale values to a standard deviation of one.\n",
    "\n",
    "Note: Fitting _must_ be done on the train data to avoid 'leaking' from test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now try without scaling numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "col_names = ['total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim']\n",
    "\n",
    "#ct = ColumnTransformer([('sc', StandardScaler(), col_names)], remainder='passthrough')\n",
    "\n",
    "#X_train = ct.fit_transform(X_train)\n",
    "#X_test = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature creation:\n",
    "__One-hot-encoding__ <br>\n",
    "Categorical integer features should be transformed into “one-hot” vectors. In relational terms this results in addition of additional columns – one column for each distinct category.\n",
    "\n",
    "Note: this can be done on data before spliting into train and test to cover all categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding mismatch between train and test after one-hot-encoding\n",
    "X_train['train'] = 1\n",
    "X_test['train'] = 0\n",
    "\n",
    "# concat train and test set\n",
    "combined = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# one-hot-encode\n",
    "combined = pd.get_dummies(combined, drop_first=True)\n",
    "\n",
    "# split back into train and test, drop column train\n",
    "X_train = combined[combined['train']==1]\n",
    "X_test = combined[combined['train']==0]\n",
    "X_train.drop(['train'], axis=1, inplace=True)\n",
    "X_test.drop(['train'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Time-to-Frequency transformation__ <br>\n",
    "Time-series (and sometimes also sequence data) is recorded in the time domain but can easily transformed into the frequency domain e.g. using FFT (Fast Fourier Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Month-From-Date__ <br>\n",
    "Creating an additional feature containing the month independent from data captures seasonal aspects. Sometimes further discretization in to quarters helps as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional column with incident_month_of_year\n",
    "X_train['incident_month_of_year'] = X_train['incident_date'].dt.month\n",
    "X_test['incident_month_of_year'] = X_test['incident_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional column with incident_day_of_year\n",
    "X_train['incident_day_of_week'] = X_train['incident_date'].dt.dayofweek\n",
    "X_test['incident_day_of_week'] = X_test['incident_date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional to signal weekend vs. weekday\n",
    "X_train['weekday'] = X_train['incident_day_of_week'].isin([0,1,2,3,4]).astype('int')\n",
    "X_test['weekday'] = X_test['incident_day_of_week'].isin([0,1,2,3,4]).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this can be done on the data before splitting into train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aggregate-on-Target__ <br>\n",
    "Simply aggregating fields the target variable (or even other fields) can improve performance, e.g. count number of data points per ZIP code or take the median of all values by geographical region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(X_train.join(y_train), 'data/insurance_claims_train_features.csv')\n",
    "to_csv(X_test.join(y_test), 'data/insurance_claims_test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}